{
    "model_name": "mlp",
    "flatten": true,
    "seed": 233,
    "epochs": 100000,
    "best_acc": 0.0,
    "batch_size": 1024,
    "optimizer_type": "Adam",
    "optimizer": {
        "lr": 5e-6,
        "momentum": 0.9,
        "weight_decay": 5e-4
    },
    "lr_decay_epoch": [150, 225, 300],
    "lr_decay_rate": 0.1,
    "criterion": "CrossEntropyLoss",
    "trainer": "mlp_trainer"
}